{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Breakdown\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanics\n",
    "\n",
    "Fill out this notebook, save it, and **submit it electronically as described below.**\n",
    "\n",
    "On a DICE environment, open the terminal, navigate to the location of this notebook, and submit this notebook file using the following command:\n",
    "\n",
    "`submit iaml cw2 09_Assignment_4.ipynb`\n",
    "\n",
    "What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You can check the status of your submissions with the `show_submissions` command.\n",
    "\n",
    "**Distance Learners:** To copy your work up to DICE (such that you can use the `submit` command) you can use `scp` or `rsync` (you may need to install these yourself). You can copy files up using `student.ssh.inf.ed.ac.uk`, then ssh in to submit, e.g. (in a unix terminal):\n",
    "```\n",
    "filename=09_Assignment_4.ipynb\n",
    "local_scp_filepath=~/git/iaml2017/${filename}\n",
    "UUN=s0816700\n",
    "server_address=student.ssh.inf.ed.ac.uk\n",
    "scp -r ${local_scp_filepath} ${UUN}@${server_address}:${filename}\n",
    "# rsync -rl ${local_scp_filepath} ${UUN}@${server_address}:${filename}\n",
    "ssh ${UUN}@${server_address}\n",
    "ssh student.login\n",
    "submit iaml cw1 09_Assignment_4.ipynb\n",
    "```\n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics MSc Degree Guide is that normally you will not be allowed to submit coursework late. See http://www.inf.ed.ac.uk/teaching/years/msc/courseguide10.html#exam for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you should NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any).\n",
    "\n",
    "**Resubmission:** If you submit your file again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/JamesOwers/iaml2017) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate iaml\n",
    "cd iaml_2017\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (the `datasets` directory is adjacent to this file).\n",
    "\n",
    "1. **IMPORTANT:** Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this assignment you are asked to import all the packages and modules you will need. Include all required imports and execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the datasets\n",
    "\n",
    "\n",
    "This assignment is based on two datasets:\n",
    "1. the 20 Newsgroups Dataset (you should recognise it from Assignment 1)\n",
    "2. the MNIST digits dataset\n",
    "\n",
    "### 20 Newsgroups\n",
    "\n",
    "For convenience, we repeat the description here. This dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware, comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale, soc.religion.christian). \n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We will use documents from only 5 out of the 20 newsgroups, which results in a 5-class problem. More specifically the 5 classes correspond to the following newsgroups: \n",
    "1. `alt.atheism`\n",
    "2. `comp.sys.ibm.pc.hardware`\n",
    "3. `comp.sys.mac.hardware`\n",
    "4. `rec.sport.baseball`\n",
    "5. `rec.sport.hockey `\n",
    "\n",
    "However, note here that classes 2-3 and 4-5 are rather closely related.\n",
    "\n",
    "**In contrast to Assignment 1**, we have opted to use tf-idf weights ([term frequency - inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))\n",
    "for each word instead of the frequency counts. These weights represent the importance of a word to a\n",
    "document with respect to a collection of documents. The importance increases proportionally to the number\n",
    "of times a word appears in the document and decreases proportionally to the number of times the word\n",
    "appears in the whole corpus. \n",
    "\n",
    "Additionally we preprocess the data to include the most frequent 1000 words that are in greater than 2 documents, less than half of all documents, and that are not [stop words](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "We will perform all this preprocessing for you.\n",
    "\n",
    "\n",
    "### MNIST\n",
    "This MNIST Dataset is a collection handwritten digits. The samples are partitioned (nearly) evenly across the 10 different digit classes {0, 1, . . . , 9}. We use a preprocessed version for which the data are $8 \\times 8$ pixel images containing one digit each. For further details on how the digits are preprocessed, see the sklearn documentation. The images are grayscale, with each pixel taking values in {0, 1, . . . , 16}, where 0 corresponds to black (weakest intensity) and 16 corresponds to white (strongest intensity). Therefore, the dataset is a N × 64\n",
    "dimensional matrix where each dimension corresponds to a pixel from the image and N is the number of\n",
    "images. \n",
    "\n",
    "Again, to save you time, we perfom the import for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering the 20 Newsgroups Data [50%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1 we will cluster the observations in the 20 Newsgroups dataset using the k-means algorithm. Each row of the dataset represents a document with bag of words features. If we were not given the labels for each document (i.e. the newsgroup it came from), clustering could allow us to infer which documents should have the same label. Observing common words within each cluster may allow us to give meaning to these inferred labels too.\n",
    "\n",
    "First we'll import the data and fit and evaluate k-means with 5 cluster centres. Next, we will try and infer which cluster corresponds with which label. Finally, we will pretend we don't know the number of clusters there should be, as is the normal scenario with large unlabeled data, and investigate the effect of using a different number of cluster centres (i.e. varying `k`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.0 --- [0 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cells to import the data. It would be a good idea to understand the code but it's not strictly necessary (see the [sklearn documentation](http://scikit-learn.org/0.17/datasets/index.html#the-20-newsgroups-text-dataset)).\n",
    "\n",
    "*This may take a wee while as it will download the dataset*\n",
    "\n",
    "**Do not change any of the code in this question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cats = ['alt.atheism', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "        'rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000,\n",
    "                             min_df=2, stop_words='english', \n",
    "                             token_pattern='(?u)\\\\b[A-z]{2,}\\\\b')\n",
    "X_sparse = vectorizer.fit_transform(newsgroups_train.data)\n",
    "y_num = newsgroups_train.target\n",
    "X = pd.DataFrame(X_sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "y = np.array(cats)[y_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 --- [5 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an initial inspection of the data, X and y (no more than 5 lines of code). Below the code, describe what the data are i.e. what the objects are, and what they represent (fewer than 4 sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>ac</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>according</th>\n",
       "      <th>acquired</th>\n",
       "      <th>actually</th>\n",
       "      <th>adaptec</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yankees</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "      <td>2845.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.001626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.030224</td>\n",
       "      <td>0.020739</td>\n",
       "      <td>0.034631</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>0.026958</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.026026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026988</td>\n",
       "      <td>0.039172</td>\n",
       "      <td>0.032786</td>\n",
       "      <td>0.031238</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.039346</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.025381</td>\n",
       "      <td>0.030929</td>\n",
       "      <td>0.022175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.723914</td>\n",
       "      <td>0.420916</td>\n",
       "      <td>0.431380</td>\n",
       "      <td>0.519765</td>\n",
       "      <td>0.443354</td>\n",
       "      <td>0.663808</td>\n",
       "      <td>0.572124</td>\n",
       "      <td>0.413233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723081</td>\n",
       "      <td>0.568146</td>\n",
       "      <td>0.878185</td>\n",
       "      <td>0.693531</td>\n",
       "      <td>0.650649</td>\n",
       "      <td>0.455493</td>\n",
       "      <td>0.646447</td>\n",
       "      <td>0.572455</td>\n",
       "      <td>0.703771</td>\n",
       "      <td>0.605576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               abc      ability         able           ac       accept  \\\n",
       "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
       "mean      0.002436     0.002149     0.006709     0.001454     0.002286   \n",
       "std       0.030224     0.020739     0.034631     0.023100     0.021634   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       0.723914     0.420916     0.431380     0.519765     0.443354   \n",
       "\n",
       "            access    according     acquired     actually      adaptec  \\\n",
       "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
       "mean      0.003953     0.003115     0.000921     0.008366     0.001816   \n",
       "std       0.030111     0.026958     0.016275     0.043921     0.026026   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       0.663808     0.572124     0.413233     1.000000     0.578282   \n",
       "\n",
       "          ...           written        wrong      yankees         yeah  \\\n",
       "count     ...       2845.000000  2845.000000  2845.000000  2845.000000   \n",
       "mean      ...          0.002612     0.006811     0.002470     0.003564   \n",
       "std       ...          0.026988     0.039172     0.032786     0.031238   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "max       ...          0.723081     0.568146     0.878185     0.693531   \n",
       "\n",
       "              year        years          yes         york        young  \\\n",
       "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
       "mean      0.016934     0.008797     0.006381     0.002772     0.003288   \n",
       "std       0.061181     0.039346     0.037319     0.025381     0.030929   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       0.650649     0.455493     0.646447     0.572455     0.703771   \n",
       "\n",
       "              zone  \n",
       "count  2845.000000  \n",
       "mean      0.001626  \n",
       "std       0.022175  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       0.605576  \n",
       "\n",
       "[8 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>ac</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>according</th>\n",
       "      <th>acquired</th>\n",
       "      <th>actually</th>\n",
       "      <th>adaptec</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yankees</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abc  ability  able   ac  accept  access  according  acquired  actually  \\\n",
       "0  0.0      0.0   0.0  0.0     0.0     0.0        0.0       0.0       0.0   \n",
       "1  0.0      0.0   0.0  0.0     0.0     0.0        0.0       0.0       0.0   \n",
       "2  0.0      0.0   0.0  0.0     0.0     0.0        0.0       0.0       0.0   \n",
       "3  0.0      0.0   0.0  0.0     0.0     0.0        0.0       0.0       0.0   \n",
       "4  0.0      0.0   0.0  0.0     0.0     0.0        0.0       0.0       0.0   \n",
       "\n",
       "   adaptec  ...   written  wrong  yankees  yeah  year  years  yes  york  \\\n",
       "0      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
       "1      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
       "2      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
       "3      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
       "4      0.0  ...       0.0    0.0      0.0   0.0   0.0    0.0  0.0   0.0   \n",
       "\n",
       "   young  zone  \n",
       "0    0.0   0.0  \n",
       "1    0.0   0.0  \n",
       "2    0.0   0.0  \n",
       "3    0.0   0.0  \n",
       "4    0.0   0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2845, 1000), (2845,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
       "       'rec.sport.baseball', 'rec.sport.hockey'],\n",
       "      dtype='|S24')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*\n",
    "\n",
    "X is a dataframe with 1000 attributes (columns), each representing a word. Each data point / row represents a document. \n",
    "X[row,colum] will contain the tf-idf weight for a word (column) for a document (row). It appears most of these weights are 0 as would be expected from a bad of words model.\n",
    "\n",
    "y is a numpy array containing class labels for documents (rows/data points) in X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 --- [2 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) object with 5 clusters. Make sure that you can reproduce your results exactly. *Hint: there is an argument for this*. You need only set two arguments; others can be kept as default. Call the instantiated object `kmeans`. Use the `fit()` method to fit to the training data (X imported above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=1337, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "kmeans = KMeans(n_clusters=5,random_state=1337)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [6 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evauate the quality of this fit.\n",
    "\n",
    "1. Get a feel for the average distance of a data point from the centre: print the mean of the euclidean distances of all data points from the mean data point (in the whole dataset)\n",
    "1. print the inertia of the model. *Hint: check the properties of the kmeans object*\n",
    "1. print the adjusted rand index of the model. *Hint: `adjusted_rand_score`*\n",
    "\n",
    "Below the code: \n",
    "1. Define what the inertia and adjusted rand score are (one or two sentences). *Hint: check [sklearn documentation](http://scikit-learn.org/stable/modules/clustering.html)*\n",
    "1. Comment on the quality of the clustering implied by the adjusted rand score and inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of - euclidian distances of all data point from mean data point is 0.954584949852.\n",
      "Interia of model is 2593.37189411\n",
      "Adjusted rand index of the model is 0.250608658757\n"
     ]
    }
   ],
   "source": [
    "mean_data_point = X.mean(axis=0)\n",
    "distances = []\n",
    "for x in X.values:\n",
    "    distances.append(np.linalg.norm(x-mean_data_point))\n",
    "print('Mean of - euclidian distances of all data point from mean data point is {}.'.format(np.mean(distances)))\n",
    "print('Interia of model is {}'.format(kmeans.inertia_))\n",
    "print('Adjusted rand index of the model is {}'.format(adjusted_rand_score(y, kmeans.labels_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia: The kmeans algorithm tries to minimize a criterion called inertia, or the within-cluster sum of squares criterion. It is a measure of how internally coherent clusters are. \n",
    "\n",
    "Adjusted Rand Score: Adjusted Rand index is a function that measures the similarity of the two assignments, ignoring permutations and with chance normalization. \n",
    "\n",
    "ARI suggests that we have  not performed well at clustering data in clusters such that they match out true classes. However, as mentioned above, some classes are related to each other and perhaps the clusters represent that structure. Inertia alone won't suggest much unless we have other clusters on the same data present and we can compare their inertia then. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print:\n",
    "1. the number of datapoints with each label\n",
    "2. the number of datapoints assigned to each cluster. *Hint: you should use the properties of the kmeans object you just fit.* \n",
    "\n",
    "Below the code, comment on the distribution of datapoints to cluster centres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class rec.sport.baseball: 597\n",
      "Class alt.atheism: 480\n",
      "Class comp.sys.mac.hardware: 578\n",
      "Class comp.sys.ibm.pc.hardware: 590\n",
      "Class rec.sport.hockey: 600\n",
      "\n",
      "Number of datapoints in cluster 1 is 969\n",
      "Number of datapoints in cluster 2 is 689\n",
      "Number of datapoints in cluster 3 is 222\n",
      "Number of datapoints in cluster 4 is 381\n",
      "Number of datapoints in cluster 5 is 584\n"
     ]
    }
   ],
   "source": [
    "d = dict()\n",
    "for label in y:\n",
    "    if d.has_key(label):\n",
    "        d[label]= d[label]+1\n",
    "    else:\n",
    "        d[label] = 1\n",
    "\n",
    "for cls,num in d.items():\n",
    "    print('Class {}: {}'.format(cls,num))\n",
    "\n",
    "print('')    \n",
    "\n",
    "d2 = dict()\n",
    "for cls in kmeans.labels_:\n",
    "    if d2.has_key(cls):\n",
    "        d2[cls] = d2[cls]+1\n",
    "    else:\n",
    "        d2[cls] = 1\n",
    "for cls,num in d2.items():\n",
    "    print('Number of datapoints in cluster {} is {}'.format(cls+1,num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters formed are not representative of our class labels as the distribution of datapoints in clusters do not agree with distribution of datapoints in y (true labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't visualise these cluster centres directly, since they are 1000 dimensional. However, we can at least measure the distance between each centre. Create a distance matrix such that the entry with index (i,j) shows the distance between centre i and j. *Hint: again you should use the properties of the kmeans object you just fit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.18862704  0.34253171  0.21613938  0.20554353]\n",
      " [ 0.18862704  0.          0.32152977  0.27272268  0.29016893]\n",
      " [ 0.34253171  0.32152977  0.          0.39789789  0.40913504]\n",
      " [ 0.21613938  0.27272268  0.39789789  0.          0.28301014]\n",
      " [ 0.20554353  0.29016893  0.40913504  0.28301014  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "distance_matrix = np.zeros((5,5))\n",
    "for i,centre in enumerate(kmeans.cluster_centers_):\n",
    "    for j,other_centre in enumerate(kmeans.cluster_centers_):\n",
    "        distance_matrix[i,j] = np.linalg.norm(centre-other_centre)\n",
    "\n",
    "print(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.6 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster centre label, plot the counts of the true labels. The cluster labels are a property of the k-means object, the true labels are contained in `y`. Make sure that you label the plot axes and legend clearly. Below the code, comment on the quality of the fit. *Hint: you can do the main plot (without labels) in one line with seaborn (you're free to do it as you like though!).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0wAAAHjCAYAAAAQSJ+lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9a0c204d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(x=kmeans.labels_, hue=y)\n",
    "plt.xlabel('cluster id')\n",
    "plt.legend(loc='center left', bbox_to_anchor=[1.1, 0.5], title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the fit is not good. However, one can see some correct fitting. Notice that pc hardware and mac hardware tend to be together in clusters 1 and 2 as they are similar. Similarly, atheism is mostly alone in cluster 3. It seems that cluster zero has a mixture of all classes (perhaps containing datapoints that this kmeans couldn't find pecularities in). In any case, the fitting is not at all ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.7 --- [8 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now investigate using dimensionality reduction to try and improve the quality of the fit. Use the sklearn implementation of [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and its method `fit_transform()` to create a reduced reduced dataset for `n_components` = [1,2,3,5,10,20,50,100,500,1000] i.e. create datasets that are of shape `(N, d)` for `d` in `n_components`. Fit k-means to each reduced dataset and report the `inertia` and `adjusted_rand_score` for each iteration.\n",
    "\n",
    "Plot `adjusted_rand_score` against number of principal components (label graph). Use a log scale on the x axis. Below the graph:\n",
    "1. describe what it shows\n",
    "1. explain why we cannot use inertia to choose the best number of principal components\n",
    "1. explain why dimensionality reduction could help k-means perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D PCA data, kmeans results\n",
      "ARI: 0.209854702983, Inertia: 2.82169843263\n",
      "\n",
      "2D PCA data, kmeans results\n",
      "ARI: 0.267077802348, Inertia: 10.5812022468\n",
      "\n",
      "3D PCA data, kmeans results\n",
      "ARI: 0.238843471073, Inertia: 20.4555658737\n",
      "\n",
      "5D PCA data, kmeans results\n",
      "ARI: 0.224103755572, Inertia: 49.8387429409\n",
      "\n",
      "10D PCA data, kmeans results\n",
      "ARI: 0.216117481229, Inertia: 117.802895417\n",
      "\n",
      "20D PCA data, kmeans results\n",
      "ARI: 0.219950808394, Inertia: 230.689802166\n",
      "\n",
      "50D PCA data, kmeans results\n",
      "ARI: 0.218466992838, Inertia: 488.645833105\n",
      "\n",
      "100D PCA data, kmeans results\n",
      "ARI: 0.213898814402, Inertia: 801.415611346\n",
      "\n",
      "500D PCA data, kmeans results\n",
      "ARI: 0.185318389993, Inertia: 2132.21419201\n",
      "\n",
      "1000D PCA data, kmeans results\n",
      "ARI: 0.213144079103, Inertia: 2593.25215136\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhUFBgiIhKV/w9QHmCZaZsyXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9a276fb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aris = []\n",
    "n_components = [1,2,3,5,10,20,50,100,500,1000]\n",
    "for ii in n_components :\n",
    "    X_pca = PCA(n_components = ii).fit_transform(X)\n",
    "    km = KMeans(n_clusters=5, random_state=1337)  \n",
    "    km.fit(X_pca)\n",
    "    y_pred = km.labels_\n",
    "    ari = adjusted_rand_score(y, y_pred)\n",
    "    aris.append(ari)\n",
    "    print('\\n{}D PCA data, kmeans results\\nARI: {}, Inertia: {}'.\\\n",
    "          format(ii, ari, km.inertia_))\n",
    "    \n",
    "plt.figure()\n",
    "plt.xscale('log')\n",
    "plt.scatter(n_components,aris)\n",
    "plt.plot(n_components,aris)\n",
    "plt.title('ARIs against PCA')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('ARI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "2. We can't use inertia to find the best number of principal components as inertia depends upon the number dimensions. As dimension increases, one can expect inertia to increase.\n",
    "3. I higher dimensions, euclidean distance is not a good distance metric. As dimensions increase, the ratio of distance between nearest and farthest point starts approaching 1. Since kmeans depends on distance, dimensionality reduction may help it perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.8 --- [6 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. So we can visualise the data, create X_2d: the dataset X transformed down to 2 principal component dimensions. Use sklearn's implementation of [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and its method `fit_transform()` to do this (as above). \n",
    "\n",
    "1. Create a new k-means object, `kmeans_pca`, and fit the 2d data to it. Show the adjusted rand score.\n",
    "\n",
    "1. As above, for each cluster centre label, plot the counts of the true labels. The cluster labels are a property of the k-means object, the true labels are contained in `y`. Make sure that you label the plot axes and legend clearly. Print below it the number of data points each cluster is responsible for.\n",
    "\n",
    "1. Finally, below the plot, comment on the difference between these clusters and the clusters on the 1000 dimensional data with respect to the distribution of the labels in each. Are they better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.9 --- [1 mark] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above plot, apply a label to each cluster centre. Create a new vector, `labels_interp`, which is the same as `kmeans.labels_`, but instead of numbers, the interpreted label of the cluster centre. For example, if you decided cluster 0 was 'apples', 1 was 'pears', 2 was 'stairs', and `kmeans.labels_` was `[2, 0, 1, 1, 0]`, create  `labels_interp = ['stairs', 'apples', 'pears', 'pears', 'apples']`. Hint: an example of how to do this is given in the last line of Question 1.0.\n",
    "\n",
    "**N.B. be careful to use the `kmeans_pca` object you created above, not the first kmeans object you made**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.10 --- [3 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a normalised (by true label) confusion matrix of your interpreted labels from the k-means clustering and the true labels. As usual, you may use any functions from previous assignments or labs. Clearly label the axes of the plot. Check that these confusions correlate with your expectations! N.B. this is just a slightly different way of interpreting the information in the count plot above (focussed this time on the true labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.11 --- [6 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are now in 2 dimensions, let's visualise the data, the cluster centres, and the decision boundaries via a [Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram). You'll essentially be able to copy and paste the code from the [sklearn kmeans digits example](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) and edit a few bits to get over half marks.\n",
    "\n",
    "Clearly mark each cluster centre.\n",
    "\n",
    "For full marks, additionally:\n",
    "* label each cluster centre with the inferred cluster label\n",
    "* create a second plot which clearly shows where the true classes lie within the pca space e.g. the [sklearn PCA example here](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.12 --- [4 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a **1 or 2 sentence** answer for each question*.\n",
    "\n",
    "1. Is the kmeans algorithm deterministic?\n",
    "1. Did the algorithm you ran above in Question 1.2 (fitting k-means) do multiple initialisations? If not, explain why it was not necessary. If so, how was the final model selected?\n",
    "1. The default method for initialising centres in the sklearn implementation is [kmeans++](https://en.wikipedia.org/wiki/K-means%2B%2B). Name another method for initialising and a problem with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA with MNIST Data [50%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the second part of the assignment we will explore the MNIST digits dataset. We expect the digits to lie in a lower-dimensional manifold and want to examine the representation we get by applying Principal Components Analysis (PCA). PCA maps the data into a new space by effectively rotating the base vectors of the input space to the directions with the highest variance. We will assess the impact of this mapping to the classification task and the separability of the data in the PCA space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.0 --- [0 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to load the digits object and print its description.\n",
    "\n",
    "**Do not change any of the code in this question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 --- [8 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute `digits.keys()`, you'll see this object contains the data, the targets (the labels), and the images (the data reshaped to 8x8 images). More typically the mnist data are [28x28 images](http://yann.lecun.com/exdb/mnist/), but this is a preprocessed version. \n",
    "\n",
    "1. Use the pandas describe method on the data to get a feel for the range of each dimension\n",
    "1. What are the max/min values for each dimension?\n",
    "1. Extract the standard deviations from the output of the describe method (just another DataFrame with 'std' as one of the index values), reshape to an 8x8 image, and plot a heatmap (use `sns.heatmap()`) to show you which dimensions vary the most. For a bonus mark, produce a plot like this for each digit. *Hint: you can use `.groupby(digits.target)` before calling `.describe()`*.\n",
    "1. Use `sns.heatmap()` to plot the first 9 digits in the dataset\n",
    "\n",
    "Below the plots, answer this question in a markdown cell:\n",
    "1. Are all of the dimensions going to be equally helpful for modelling? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 --- [10 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new `PCA` object with `n_components = digits.data.shape[1]`. Plot the explained variance **ratio** against the number of components. You may find [this example](http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py) quite handy...\n",
    "\n",
    "Find the point where 95% of the variance has been explained. Use `plt.vlines()` to add a dotted verical line to the graph at that point and use `plt.annotate()` to label the number of eigenvectors used to explain that variance.\n",
    "\n",
    "Below the plot, explain what you observe. What does this suggest about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 --- [8 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the eigenvectors stored within `pca.components_`. Reuse your code from Question 2.1 and plot the first 9 principal components (PCs). Below, plot `pca.mean_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 --- [10 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make clear how these eigenvectors and the mean are used to approximate the data. `pca.transform(digits.data[idx,:].reshape(1, -1)).flatten()` will transform the digit in row `idx` into pca space. Another way to think about this is that it will give you the coefficents to multiply each eigenvector by and to add to the mean digit such that you can reconstruct the digit.\n",
    "\n",
    "For the digit with idx = 0 (which should itself be a zero), create 4 plots:\n",
    "1. The original digit\n",
    "1. The digit reconstructed using 1 principal component\n",
    "1. The digit reconstructed using 2 principal components\n",
    "1. The digit reconstructed using 5 principal components\n",
    "\n",
    "In the plot titles show:\n",
    "1. the number of principal components used\n",
    "1. the percentage of variance explained by that number of principal components\n",
    "1. the coefficients of each principal component rounded to nearest integer (tip, convert to integers to reduce print space), i.e. the PCA space vector.\n",
    "\n",
    "Below the plots, comment on the result. Do the eigenvectors produce realistic data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.5 --- [14 marks] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge question** \n",
    "\n",
    "Fit 4 models:\n",
    "1. SVC with a linear kernel, no shrinking, and a random_state on digit data\n",
    "1. SVC with a linear kernel, no shrinking, and the same random_state on pca transformed digit data (use the full 64 component pca above)\n",
    "1. Gaussian Naive Bayes Classifier on digit data\n",
    "1. Gaussian Naive Bayes Classifier on pca transformed digit data (use the full 64 component pca above)\n",
    "\n",
    "Use 5 fold cross validation and take the mean fold score as the result. Plot or print the results.\n",
    "\n",
    "Below the code, explain why one classifier improved when we used PCA, but the other did not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
